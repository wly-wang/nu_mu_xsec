{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDT\n",
    "\n",
    "This Jupyter Notebook contains the BDT used for separating the muon neutrino and muon anti-neutrino in the mixed sample containing $\\nu_{\\mu} + \\bar{\\nu}_{\\mu}$. \n",
    "\n",
    "Input taking the .hdr5 files containing the Pandas dataframes for the selected sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input, importing the selected sample stored into .hdr5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================#\n",
    "#  Import Packages   #\n",
    "#====================#\n",
    "\n",
    "import math \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import uproot3 as uproot\n",
    "\n",
    "# BDT\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, recall_score, precision_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================#\n",
    "#  Import dataframes containing sample of mostly mixed numu anti-numu #\n",
    "#=====================================================================#\n",
    "\n",
    "\n",
    "df_mc = pd.read_hdf('dfs/df_run1_mc_mixed_numu_numubar.hdf5')\n",
    "df_beam_on = pd.read_hdf('dfs/df_run1_data_mixed_numu_numubar.hdf5')\n",
    "df_dirt = pd.read_hdf('dfs/df_run1_dirt_mixed_numu_numubar.hdf5')\n",
    "df_ext = pd.read_hdf('dfs/df_run1_ext_mixed_numu_numubar.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================#\n",
    "# Auxiliary functions used for BDT #\n",
    "#==================================#\n",
    "\n",
    "\n",
    "def label_signal(df_):\n",
    "    # this function labels the sample, =1 for signal (numubarCC events), and =0 for background (otherwise) # flip definition\n",
    "      \n",
    "    condition_numubar = (df_['truth_nuPdg']==-14)\n",
    "    condition_numu = (df_['truth_nuPdg']==14)\n",
    "    condition_cc = (df_['truth_isCC']==1)\n",
    "    \n",
    "    query_numubarcc = condition_numubar & condition_cc\n",
    "    query_numucc = condition_numu & condition_cc\n",
    "    \n",
    "    df_[\"bdt_numubarcc_label\"] = df_.apply(lambda row:1 if query_numubarcc[row.name] else 0, axis=1)\n",
    "    df_[\"bdt_numucc_label\"] = df_.apply(lambda row:1 if query_numucc[row.name] else 0, axis=1)\n",
    "    \n",
    "    #Calculate the percentage of 1's\n",
    "    percentage_signal_numu = (df_[\"bdt_numucc_label\"].sum() / len(df_)) * 100\n",
    "    print(f\"Labelled dataset. Portion of signal numuCC events in this sample: {percentage_signal_numu:.2f}%\")\n",
    "    \n",
    "    percentage_signal_numubar = (df_[\"bdt_numubarcc_label\"].sum() / len(df_)) * 100\n",
    "    print(f\"Labelled dataset. Portion of signal numubarCC events in this sample: {percentage_signal_numubar:.2f}%\")\n",
    "    \n",
    "def remove_background_events(df_):\n",
    "    # remove any event that is not nueCC/nuebarCC\n",
    "    query = \"(truth_nuPdg==14 | truth_nuPdg==-14) & truth_isCC==1\"\n",
    "    num_events_before = len(df_)\n",
    "    df_.query(query, inplace=True) # keep only events where query=True\n",
    "    contaminant_contribution = (num_events_before - len(df_))/num_events_before\n",
    "    print(f\"Removed {num_events_before - len(df_)} contaminants from the selection sample. This\"\n",
    "          f\" constitutes {contaminant_contribution:.2%} of your sample. If this is large, consider refining the\"\n",
    "          f\" initial selection process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled dataset. Portion of signal numuCC events in this sample: 73.83%\n",
      "Labelled dataset. Portion of signal numubarCC events in this sample: 17.00%\n",
      "Removed 2545 contaminants from the selection sample. This constitutes 9.17% of your sample. If this is large, consider refining the initial selection process.\n",
      "Scaling minority class events by factor = 1.00e+00\n",
      "[0]\tvalidation_0-error:0.22256\tvalidation_0-auc:0.81729\tvalidation_0-logloss:0.68969\tvalidation_1-error:0.30571\tvalidation_1-auc:0.61661\tvalidation_1-logloss:0.69070\n",
      "[1]\tvalidation_0-error:0.24900\tvalidation_0-auc:0.81992\tvalidation_0-logloss:0.68717\tvalidation_1-error:0.34298\tvalidation_1-auc:0.61608\tvalidation_1-logloss:0.68919\n",
      "[2]\tvalidation_0-error:0.26319\tvalidation_0-auc:0.82033\tvalidation_0-logloss:0.68470\tvalidation_1-error:0.35170\tvalidation_1-auc:0.61583\tvalidation_1-logloss:0.68772\n",
      "[3]\tvalidation_0-error:0.26468\tvalidation_0-auc:0.82062\tvalidation_0-logloss:0.68228\tvalidation_1-error:0.35329\tvalidation_1-auc:0.61569\tvalidation_1-logloss:0.68632\n",
      "[4]\tvalidation_0-error:0.26508\tvalidation_0-auc:0.82147\tvalidation_0-logloss:0.67988\tvalidation_1-error:0.35646\tvalidation_1-auc:0.61687\tvalidation_1-logloss:0.68487\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wwang/nu_mu_xsec/BDT.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Train the classifier on our newly constructed dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m eval_set \u001b[39m=\u001b[39m [(features_train, targets_train), (features_test, targets_test)]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(features_train, targets_train, sample_weight\u001b[39m=\u001b[39;49msample_weights_train, eval_set\u001b[39m=\u001b[39;49meval_set)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# ============================\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Calculate bdt score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/wwang/nu_mu_xsec/BDT.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m df_bdt\u001b[39m.\u001b[39mloc[:,chosen_var] \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(df_bdt[training_variables])[:,\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/BDT/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/BDT/lib/python3.11/site-packages/xgboost/sklearn.py:1515\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1487\u001b[0m (\n\u001b[1;32m   1488\u001b[0m     model,\n\u001b[1;32m   1489\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1495\u001b[0m )\n\u001b[1;32m   1496\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1497\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1498\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1513\u001b[0m )\n\u001b[0;32m-> 1515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1516\u001b[0m     params,\n\u001b[1;32m   1517\u001b[0m     train_dmatrix,\n\u001b[1;32m   1518\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1519\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1520\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1521\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1522\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1523\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1524\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1525\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1526\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1527\u001b[0m )\n\u001b[1;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1530\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/BDT/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/BDT/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BDT/lib/python3.11/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     _check_call(\n\u001b[0;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[1;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[1;32m   2052\u001b[0m         )\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Rename dataframe, allowing df_bdt to be other things in the future\n",
    "df_bdt = df_mc\n",
    "\n",
    "# Label the sample provided, include \"bdt_nuebarcc_label\" and \"bdt_nuecc_label\"\n",
    "label_signal(df_bdt)\n",
    "\n",
    "# Remove remaining background (everything different than nueCC/nuebarCC)\n",
    "remove_background_events(df_bdt)\n",
    "\n",
    "# Calculate extra variables\n",
    "#calc_vertex_dist(df_bdt)\n",
    "#calc_costheta(df_bdt)\n",
    "\n",
    "# ============================\n",
    "\n",
    "#chosen_var = 'bdt_nuebarcc_label' #nuebarcc=signal, nuecc=background\n",
    "chosen_var = 'bdt_numucc_label' # nuecc=signal, nuebarcc=background\n",
    "\n",
    "# ============================\n",
    "\n",
    "# List of variables used for training\n",
    "training_variables = ['showervtx_diff','gap_energy','gap_flag_single_shower','gap_flag','reco_showerKE', 'cos_theta']\n",
    "#training_variables = ['cos_theta']\n",
    "\n",
    "# Split the dataset into training and validation sets - using random_state as handle on the randomness of data\n",
    "features = df_bdt[training_variables]\n",
    "targets = df_bdt[chosen_var]\n",
    "\n",
    "# Compute sample weights to balance the class distribution\n",
    "sample_weights = compute_sample_weight(class_weight=\"balanced\", y=targets)\n",
    "features_train, features_test, targets_train, targets_test, sample_weights_train, sample_weights_test = train_test_split(features, targets, sample_weights, test_size=0.1, random_state=0)\n",
    "\n",
    "# Calculate the scale by which to increase the gradient for minority class examples\n",
    "#scale = minority_gradient_scaler(df_bdt.copy())\n",
    "scale = 1\n",
    "print('Scaling minority class events by factor = %.2e' % scale)\n",
    "\n",
    "# Create an XGBoost classifier (we should play with these parameters)\n",
    "clf = xgb.XGBClassifier(n_estimators=400, # 500\n",
    "                        eta=0.05, # 0.05\n",
    "                        max_depth=30, # 30\n",
    "                        learning_rate=0.01,\n",
    "                        scale_pos_weight=1, \n",
    "                        eval_metric=[\"error\",\"auc\", \"logloss\"],\n",
    "                        objective = 'binary:logistic',\n",
    "                        use_label_encoder=False # removes warning\n",
    "                       )\n",
    "\n",
    "# Train the classifier on our newly constructed dataset\n",
    "eval_set = [(features_train, targets_train), (features_test, targets_test)]\n",
    "clf.fit(features_train, targets_train, sample_weight=sample_weights_train, eval_set=eval_set)\n",
    "\n",
    "# ============================\n",
    "\n",
    "# Calculate bdt score\n",
    "df_bdt.loc[:,chosen_var] = clf.predict_proba(df_bdt[training_variables])[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock4 = False\n",
    "if lock4==True: \n",
    "    rcParams['figure.figsize'] = (100,100)\n",
    "    xgb.plot_tree(clf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out relevant evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant information for printing\n",
    "\n",
    "results = clf.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = np.linspace(0, epochs, 400, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('XGBoost AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel('Log-Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('XGBoost Log-Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "plt.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "plt.legend()\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('XGBoost Classification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def buildROC(target_test, test_preds, target_train, train_preds):\n",
    "    fpr_test, tpr_test, _ = metrics.roc_curve(target_test, test_preds)\n",
    "    fpr_train, tpr_train, _ = metrics.roc_curve(target_train, train_preds)\n",
    "    roc_auc_test = metrics.auc(fpr_test, tpr_test)\n",
    "    roc_auc_train = metrics.auc(fpr_train, tpr_train)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr_test, tpr_test, label = 'Test - AUC = %0.2f' % roc_auc_test, color='g')\n",
    "    plt.plot(fpr_train, tpr_train, label = 'Train - AUC = %0.2f' % roc_auc_train, color='b')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_train = clf.predict_proba(features_train)\n",
    "y_pred_proba_test = clf.predict_proba(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildROC(targets_test, y_pred_proba_test[:, 1], targets_train, y_pred_proba_train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
