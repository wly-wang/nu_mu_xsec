{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d47cd7e",
   "metadata": {},
   "source": [
    "This code is adapted from Marina's NuMI $\\nu_e$ and $\\bar{\\nu}_e$ xsec analysis for the use of a similar NuMI $\\nu_\\mu$ and $\\bar{\\nu}_\\mu$ xsec analysis. \n",
    "\n",
    "There are two stages:\n",
    "* Separate $\\nu_\\mu$ and $\\bar{\\nu}_\\mu$ by trainin a BDT, based on the BDT Marina developed for her analysis.\n",
    "* Then, individual $\\nu_\\mu$ and $\\bar{\\nu}_\\mu$ xsec will be measure using MicroBooNE off-axis NuMI beam data (run 1 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428a39ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwang/anaconda3/envs/BDT/lib/python3.11/site-packages/awkward0/__init__.py:8: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(numpy.__version__) < distutils.version.LooseVersion(\"1.13.1\"):\n"
     ]
    }
   ],
   "source": [
    "#====================#\n",
    "#  Import Packages   #\n",
    "#====================#\n",
    "\n",
    "import math \n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import uproot3 as uproot\n",
    "\n",
    "#Machine learning packages\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30ed95",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1552448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#  Define the boundaries of the TPCc volume #\n",
    "#===========================================#\n",
    "\n",
    "tpc_xmin = -1.55\n",
    "tpc_xmax = 254.8\n",
    "tpc_ymin = -115.53\n",
    "tpc_ymax = 117.47\n",
    "tpc_zmin = 0.1\n",
    "tpc_zmax = 1036.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0f9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================#\n",
    "#  A function for creating dataframes and importing data #\n",
    "#  from root files                                       #\n",
    "#========================================================#\n",
    "def new_df(file, fType, pot_data, pot_ext, pot_mc, pot_dirt):\n",
    "    \n",
    "    \n",
    "    #—————————————————————————————————————————————————\n",
    "    # First, define variables needed from the WC trees\n",
    "    #—————————————————————————————————————————————————\n",
    "    \n",
    "    \n",
    "    #reco vars\n",
    "    eval_vars_reco = ['flash_time', 'match_isFC']\n",
    "    pfeval_vars_reco = ['reco_nuvtxX','reco_nuvtxY', 'reco_nuvtxZ', 'run', 'subrun',\n",
    "                        'event', 'reco_muonMomentum']\n",
    "    bdt_vars_reco = ['numu_cc_flag']\n",
    "    kine_vars_reco = ['kine_reco_Enu', 'kine_pio_mass','kine_pio_flag','kine_pio_vtx_dis',\n",
    "                      'kine_pio_energy_1','kine_pio_energy_2','kine_pio_dis_1',\n",
    "                      'kine_pio_dis_2','kine_pio_angle'] #pi0 selection\n",
    "    \n",
    "    #true vars\n",
    "    kine_vars_true = []\n",
    "    bdt_vars_true = []\n",
    "    pfeval_vars_true = ['truth_corr_nuvtxX', 'truth_corr_nuvtxY', 'truth_corr_nuvtxZ', \n",
    "                        'truth_nuIntType', 'truth_pdg', 'truth_NprimPio']\n",
    "    eval_vars_true = ['weight_cv', 'weight_spline', 'truth_nuPdg', 'truth_isCC', \n",
    "                      'truth_nuEnergy','truth_vtxX', 'truth_vtxY', 'truth_vtxZ',\n",
    "                      'match_completeness_energy', 'truth_energyInside', 'truth_vtxInside']\n",
    "    \n",
    "    \n",
    "    #————————————————————————————————————\n",
    "    # import the trees from the root file\n",
    "    #————————————————————————————————————\n",
    "    \n",
    "    \n",
    "    T_eval = uproot.open(file)['wcpselection/T_eval']\n",
    "    T_PFeval = uproot.open(file)['wcpselection/T_PFeval']\n",
    "    T_BDTvars = uproot.open(file)['wcpselection/T_BDTvars']\n",
    "    T_KINEvars = uproot.open(file)['wcpselection/T_KINEvars']\n",
    "\n",
    "    \n",
    "    #———————————————————————————————————————————————————\n",
    "    # for each tree, create a reco and a true data frame\n",
    "    #———————————————————————————————————————————————————\n",
    "    \n",
    "    \n",
    "    #Reco dfs\n",
    "    df_eval_reco = T_eval.pandas.df(eval_vars_reco, flatten=False)\n",
    "    df_PFeval_reco = T_PFeval.pandas.df(pfeval_vars_reco, flatten=False)\n",
    "    df_BDTeval_reco = T_BDTvars.pandas.df(bdt_vars_reco, flatten=False)\n",
    "    df_KINEvars_reco =T_KINEvars.pandas.df(kine_vars_reco, flatten=False)\n",
    "    \n",
    "    #use concat to merge the individual reco dataframes\n",
    "    df_ = pd.concat([df_KINEvars_reco, df_BDTeval_reco, df_PFeval_reco, \n",
    "                     df_eval_reco], axis=1)\n",
    "    \n",
    "    #true dfs\n",
    "    if((fType=='MC') | (fType=='DIRT')):\n",
    "        \n",
    "        #for each tree, create a true data frame\n",
    "        df_eval_true = T_eval.pandas.df(eval_vars_true, flatten=False)\n",
    "        df_PFeval_true = T_PFeval.pandas.df(pfeval_vars_true, flatten=False)\n",
    "        df_BDTeval_true = T_BDTvars.pandas.df(bdt_vars_true, flatten=False)\n",
    "        df_KINEvars_true =T_KINEvars.pandas.df(kine_vars_true, flatten=False)\n",
    "        \n",
    "        #use concat to merge the individual dataframes\n",
    "        df_ = pd.concat([df_, df_KINEvars_true, df_BDTeval_true, df_PFeval_true, \n",
    "                         df_eval_true], axis=1)\n",
    "        \n",
    "        #fix vars, make sure they have reasonable values (weight_cv & weight_spline)\n",
    "        df_['weight_cv'] = np.where((df_.weight_cv <= 0), 1, df_.weight_cv)\n",
    "        df_['weight_cv'] = np.where((df_.weight_cv > 30), 1, df_.weight_cv)\n",
    "        df_['weight_cv'] = np.where((df_.weight_cv == np.nan), 1, df_.weight_cv)\n",
    "        df_['weight_cv'] = np.where((df_.weight_cv == np.inf), 1, df_.weight_cv)\n",
    "        df_['weight_cv'] = np.where((df_['weight_cv'].isna()), 1, df_.weight_cv)\n",
    "        df_['weight_spline'] = np.where((df_.weight_spline <= 0), 1, df_.weight_spline)\n",
    "        df_['weight_spline'] = np.where((df_.weight_spline > 30), 1, df_.weight_spline)\n",
    "        df_['weight_spline'] = np.where((df_.weight_spline == np.nan), 1, df_.weight_spline)\n",
    "        df_['weight_spline'] = np.where((df_.weight_spline == np.inf), 1, df_.weight_spline)\n",
    "        df_['weight_spline'] = np.where((df_['weight_spline'].isna()), 1, df_.weight_spline)\n",
    "        \n",
    "    \n",
    "    #————————————————\n",
    "    # only impose FHC\n",
    "    #————————————————\n",
    "    \n",
    "    \n",
    "    df_ = df_[df_.run<6748]\n",
    "    \n",
    "    \n",
    "    #————————————————————————\n",
    "    # calculate event weights\n",
    "    #————————————————————————\n",
    "    \n",
    "    \n",
    "    if(fType=='MC'):\n",
    "        W_ = pot_data/pot_mc\n",
    "        df_.loc[:,'weight_genie'] = df_['weight_cv'] * df_['weight_spline']\n",
    "        df_.loc[:,'weight_ubtune'] = [W_]*df_.shape[0] * df_['weight_genie']\n",
    "        df_.loc[:,'weight_no_ubtune'] = [W_]*df_.shape[0] * df_['weight_spline']\n",
    "    elif(fType=='DATA'): #for beam-on sample\n",
    "        W_ = 1\n",
    "        df_.loc[:,'weight_ubtune'] = [W_]*df_.shape[0]\n",
    "    elif(fType=='EXT'): #for beam-off sample\n",
    "        W_ = (pot_data/pot_ext)*0.98\n",
    "        df_.loc[:,'weight_ubtune'] = [W_]*df_.shape[0]\n",
    "    elif(fType=='DIRT'):\n",
    "        W_ = pot_data/pot_dirt\n",
    "        df_.loc[:,'weight_genie'] = df_['weight_cv'] * df_['weight_spline']\n",
    "        df_.loc[:,'weight_ubtune'] = [W_]*df_.shape[0] * df_['weight_genie']\n",
    "        df_.loc[:,'weight_no_ubtune'] = [W_]*df_.shape[0] * df_['weight_spline']\n",
    "        \n",
    "    \n",
    "    #————————————————————————————————————————————\n",
    "    # For beam-on/off data, apply a quality check\n",
    "    #————————————————————————————————————————————\n",
    "    \n",
    "    \n",
    "    if((fType=='DATA') | (fType=='EXT')):\n",
    "        df_good_runs = pd.read_csv('run1_beamon_goodquality.list', \n",
    "                sep=\", \", header=None, engine='python')\n",
    "        df_good_runs = df_good_runs.T\n",
    "        df_good_runs.rename(columns={0:'run'}, inplace=True)\n",
    "        list_good_runs = df_good_runs['run'].values.tolist()\n",
    "        df_ = df_[df_['run'].isin(list_good_runs)]\n",
    "        \n",
    "        \n",
    "    \n",
    "    #—————————————————————————————————————————————————————————————————\n",
    "    # fix neutrino energy for data\n",
    "    # the neutrino reconstructed energy needs to be corrected for data\n",
    "    # tot_energy = shower_energy + track_energy\n",
    "    # and the shower_energy needs a 95% correction factor\n",
    "    #————————————————————————————————————————————————————————————————— \n",
    "    \n",
    "    \n",
    "    if(fType=='DATA'):\n",
    "        def get_data_nuEnergyCorrected(df_, file):\n",
    "            \n",
    "            df_in = df_.copy()\n",
    "\n",
    "            # create dataframe with relevant information to \n",
    "            # calculate DATA neutrino energy\n",
    "            tree = uproot.open(file)['wcpselection/T_KINEvars']\n",
    "            var = ['kine_energy_particle', 'kine_energy_info', 'kine_particle_type', \n",
    "                   'kine_reco_add_energy', 'kine_reco_Enu']\n",
    "            df_temp = tree.pandas.df(var, flatten = True)\n",
    "\n",
    "            # apply cuts and define new dataframes\n",
    "            df_temp = df_temp[ df_temp.kine_reco_Enu>0 ]\n",
    "            df_EM = df_temp[ (df_temp.kine_energy_info==2) \\\n",
    "                            & (df_temp.kine_particle_type==11) ] # shower\n",
    "            df_track = df_temp[ (df_temp.kine_energy_info!=2) \\\n",
    "                               | (df_temp.kine_particle_type!=11) ] # track\n",
    "\n",
    "            # apply 95% correction on EM\n",
    "            df_EM.loc[:,'kine_energy_particle'] = df_EM['kine_energy_particle']\\\n",
    "                                                .apply(lambda x: x*0.95)\n",
    "\n",
    "            df_temp.loc[:,'energy_EM'] = df_EM.groupby(['entry'])['kine_energy_particle']\\\n",
    "                                        .transform('sum')\n",
    "            df_temp.loc[:,'energy_track'] = df_track.groupby(['entry'])\\\n",
    "                                        ['kine_energy_particle'].transform('sum')\n",
    "\n",
    "            df_temp['energy_EM'].fillna(0, inplace=True)\n",
    "            df_temp['energy_track'].fillna(0, inplace=True)\n",
    "\n",
    "            df_temp['energy_EM'] = df_temp.groupby(['entry'])['energy_EM'].transform(max)\n",
    "            df_temp['energy_track'] = df_temp.groupby(['entry'])['energy_track']\\\n",
    "                                        .transform(max)\n",
    "\n",
    "            df_temp = df_temp.groupby('entry').first()\n",
    "\n",
    "            df_temp['kine_reco_Enu_corr'] = df_temp['energy_EM'] + df_temp['energy_track'] \\\n",
    "                                            + df_temp['kine_reco_add_energy']\n",
    "            df_temp = df_temp.drop(['kine_energy_particle','kine_energy_info',\n",
    "                                    'kine_particle_type','kine_reco_add_energy',\n",
    "                                    'kine_reco_Enu','energy_EM','energy_track'], axis=1)\n",
    "            df_temp = df_temp.rename(columns={\"kine_reco_Enu_corr\":\"kine_reco_Enu\"})\n",
    "\n",
    "            df_in.update(df_temp)\n",
    "            return df_in\n",
    "\n",
    "            df_ = get_data_nuEnergyCorrected(df_.copy(),File)\n",
    "    \n",
    "    \n",
    "    #—————————————\n",
    "    # add RSE info\n",
    "    #—————————————\n",
    "    \n",
    "    \n",
    "    df_['RSE'] = df_[\"run\"].astype(int).apply(str) + \"_\" \\\n",
    "                + df_[\"subrun\"].astype(int).apply(str) + \"_\" \\\n",
    "                + df_[\"event\"].astype(int).apply(str)\n",
    "    \n",
    "    \n",
    "    #——————————————————————\n",
    "    # Print POT and Entries\n",
    "    #——————————————————————\n",
    "    \n",
    "    \n",
    "    if(fType=='MC'):\n",
    "        print('[%s] POT %5.2e      %7i entries' % (fType,pot_mc,len(df_)))\n",
    "    elif(fType=='DATA'): #for beam-on sample\n",
    "        print('[%s] POT %5.2e      %7i entries' % (fType,pot_data,len(df_)))\n",
    "    elif(fType=='EXT'): #for beam-off sample (TBC the pot calc)\n",
    "        print('[%s] POT %5.2e      %7i entries' % (fType,pot_ext,len(df_)))\n",
    "    elif(fType=='DIRT'):\n",
    "        print('[%s] POT %5.2e      %7i entries' % (fType,pot_dirt,len(df_)))\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2f0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================#\n",
    "#  A Function used to create pot  #\n",
    "#=================================#\n",
    "def calc_pot(file, fType):\n",
    "    \n",
    "    \n",
    "    #—————————————————————\n",
    "    #create necessary vars\n",
    "    #—————————————————————\n",
    "    \n",
    "    \n",
    "    if(fType=='MC'):\n",
    "        pot_vars = ['runNo','pot_tor875']\n",
    "    elif(fType=='DIRT'):\n",
    "        pot_vars = ['run','pot_tor875']\n",
    "    \n",
    "    \n",
    "    #——————————————————————————————————————\n",
    "    #import the pot tree from the root file\n",
    "    #——————————————————————————————————————\n",
    "    \n",
    "    \n",
    "    T_pot = uproot.open(file)['wcpselection/T_pot']\n",
    "    \n",
    "    \n",
    "    #———————————————————————————\n",
    "    #Create a pd df for pot tree\n",
    "    #———————————————————————————\n",
    "    \n",
    "    \n",
    "    df_pot = T_pot.pandas.df(pot_vars, flatten=False)\n",
    "    \n",
    "    #only impose FHC\n",
    "    if(fType=='MC'):\n",
    "        df_pot = df_pot[df_pot.runNo<6748]\n",
    "    elif(fType=='DIRT'):\n",
    "        df_pot = df_pot[df_pot.run<6748]\n",
    "    \n",
    "    \n",
    "    #—————————————————\n",
    "    #Calculate the pot\n",
    "    #—————————————————\n",
    "    \n",
    "    \n",
    "    pot = sum(df_pot.pot_tor875)\n",
    "    return pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "# A Function for calculating the angle b/w numi beam target and muon from vertex #\n",
    "#================================================================================#\n",
    "\n",
    "def calc_costheta(df):\n",
    "    \n",
    "    # position of the NuMI target\n",
    "    # those values are manually added to the lines below\n",
    "    # if you want to change them, make sure you also update the lines below!!\n",
    "    v_targ_uboone = [-31387.58422, -3316.402543, -60100.2414]\n",
    "\n",
    "    # take the vector from the NuMI target to the neutrino vertex\n",
    "    df.eval('vec_targ_vtx_X = reco_nuvtxX - (-31387.58422)', inplace=True)\n",
    "    df.eval('vec_targ_vtx_Y = reco_nuvtxY - (-3316.402543)', inplace=True)\n",
    "    df.eval('vec_targ_vtx_Z = reco_nuvtxZ - (-60100.2414)', inplace=True)\n",
    "    \n",
    "    # get reco shower vector (workaround that is working, for some reason I can't access it if I don't do this workaround, maybe it's somethind to do with the name containing [ ])\n",
    "    df.loc[:,'muon_momentum_X'] = df[\"reco_muonMomentum[0]\"]\n",
    "    df.loc[:,'muon_momentum_Y'] = df[\"reco_muonMomentum[1]\"]\n",
    "    df.loc[:,'muon_momentum_Z'] = df[\"reco_muonMomentum[2]\"]\n",
    "    df.loc[:,'muon_energy'] = df[\"reco_muonMomentum[3]\"]\n",
    "    \n",
    "    # calculate the norm of the vectors\n",
    "    df.eval('norm_vec_targ_vtx = sqrt(vec_targ_vtx_X**2 + vec_targ_vtx_Y**2 + vec_targ_vtx_Z**2)', inplace=True)\n",
    "    df.eval('norm_vec_shower = sqrt(muon_momentum_X**2 + muon_momentum_Y**2 + muon_momentum_Z**2)', inplace=True)\n",
    "    \n",
    "    # calculate cos_theta\n",
    "    df.eval('cos_theta = ((vec_targ_vtx_X * muon_momentum_X) + (vec_targ_vtx_Y * muon_momentum_Y) + (vec_targ_vtx_Z * muon_momentum_Z) )/(norm_vec_targ_vtx * norm_vec_shower)', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================#\n",
    "# A Function for calculating multiplicity  #\n",
    "#==========================================#\n",
    "\n",
    "def calc_particle_multiplicity(filename, df_original):\n",
    "\n",
    "    # --- open file and variables\n",
    "    pfeval_particle = ['reco_mother','reco_pdg','reco_Ntrack', 'run', 'subrun', 'event']\n",
    "    T_PFeval = uproot.open(filename)['wcpselection/T_PFeval']\n",
    "    DF = T_PFeval.pandas.df(pfeval_particle, flatten=True)\n",
    "\n",
    "    # --- queries to split into different variables\n",
    "    df_Neutron = DF.query('((reco_pdg==2112) | (reco_pdg==-2112)) & reco_mother==0')\n",
    "    DF.loc[:,'countNeutron'] = df_Neutron.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Muon = DF.query('((reco_pdg==13) | (reco_pdg==-13)) & reco_mother==0')\n",
    "    DF.loc[:,'countMuon'] = df_Muon.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Kaon = DF.query('((reco_pdg==321) | (reco_pdg==-321)) & reco_mother==0')\n",
    "    DF.loc[:,'countKaon'] = df_Kaon.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Pion = DF.query('((reco_pdg==211) | (reco_pdg==-211)) & reco_mother==0')\n",
    "    DF.loc[:,'countNeutron'] = df_Pion.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Proton = DF.query('((reco_pdg==2212) | (reco_pdg==-2212)) & reco_mother==0')\n",
    "    DF.loc[:,'countProton'] = df_Proton.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Gamma = DF.query('((reco_pdg==22) | (reco_pdg==-22)) & reco_mother==0')\n",
    "    DF.loc[:,'countGamma'] = df_Gamma.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_Electron = DF.query('((reco_pdg==11) | (reco_pdg==-11)) & reco_mother==0')\n",
    "    DF.loc[:,'countElectron'] = df_Electron.groupby(['entry'])['reco_pdg'].transform('count')\n",
    "\n",
    "    df_concat = DF\n",
    "\n",
    "    # --- create an extra column with the max number in the count... column\n",
    "    df_concat.loc[:,'Num_Neutron'] = df_concat.groupby(['entry'])['countNeutron'].transform(max)\n",
    "    df_concat.loc[:,'Num_Muon'] = df_concat.groupby(['entry'])['countMuon'].transform(max)\n",
    "    df_concat.loc[:,'Num_Kaon'] = df_concat.groupby(['entry'])['countKaon'].transform(max)\n",
    "    df_concat.loc[:,'Num_Proton'] = df_concat.groupby(['entry'])['countProton'].transform(max)\n",
    "    df_concat.loc[:,'Num_Gamma'] = df_concat.groupby(['entry'])['countGamma'].transform(max)\n",
    "    df_concat.loc[:,'Num_Electron'] = df_concat.groupby(['entry'])['countElectron'].transform(max)\n",
    "\n",
    "    # --- get the first subentry only --> the same as unflatten the dataframe\n",
    "    df_concat.groupby('entry').first()\n",
    "\n",
    "    # --- create the final dataframe with the columns that we want\n",
    "    df_final = df_concat[['Num_Neutron','Num_Muon','Num_Kaon','Num_Proton','Num_Gamma','Num_Electron', 'run', 'subrun', 'event']].groupby('entry').first()\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    # --- merge with the original dataframe\n",
    "    df_final = pd.merge(df_original, df_final, left_on=['run','subrun','event'], right_on=['run','subrun','event'], how='left')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b987f",
   "metadata": {},
   "source": [
    "### Event Selection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dd5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================#\n",
    "# A Function for clearing non-physical values for reco vertices #\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "def reco_nu_vtx_val_clearing(df):\n",
    "    df_ = df.query('reco_nuvtxX != -1 & reco_nuvtxY != -1 & reco_nuvtxZ != -1')\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6c8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================#\n",
    "#  A Function for Generic Neutrino Selection and NumuCC #\n",
    "#=======================================================#\n",
    "\n",
    "\n",
    "def gen_nu_selec(df):\n",
    "    selec_df = df[df.numu_cc_flag >= 1]\n",
    "    return selec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================#\n",
    "# Add Geometry vars containing Fiducial Vol Boundaries #\n",
    "#======================================================#\n",
    "fv_xmin = tpc_xmin+8.55\n",
    "fv_xmax = tpc_xmax-3.\n",
    "fv_ymin = tpc_ymin+3.\n",
    "fv_ymax = tpc_ymax-3.\n",
    "fv_zmin = tpc_zmin+3.\n",
    "fv_zmax = tpc_zmax-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64bf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================#\n",
    "#  A Function for Fiducial Volume Selection #\n",
    "#===========================================#\n",
    "\n",
    "\n",
    "def apply_inFV(df):\n",
    "    df_ = df[((df.reco_nuvtxX>(fv_xmin)) & (df.reco_nuvtxX<(fv_xmax))) &\n",
    "             ((df.reco_nuvtxY>(fv_ymin)) & (df.reco_nuvtxY<(fv_ymax))) & \n",
    "             ((df.reco_nuvtxZ>(fv_zmin)) & (df.reco_nuvtxZ<(fv_zmax)))]    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20656a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================#\n",
    "#  A Function for Containment Selection (Fully contained) #\n",
    "#=========================================================#\n",
    "\n",
    "\n",
    "def apply_isFC(df):\n",
    "    selec_df = df[df.match_isFC==1]\n",
    "    return selec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eafce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================#\n",
    "# A Function for Containment Selection (Partially contained) #\n",
    "#============================================================#\n",
    "\n",
    "\n",
    "def apply_notFC(df):\n",
    "    selec_df = df[df.match_isFC==0]\n",
    "    return selec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b234b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================#\n",
    "# A Function for applying selection cuts #\n",
    "#========================================#\n",
    "\n",
    "\n",
    "def applyCuts(label, dfData, dfEXT, dfOverlay, dfDirt, weight, print_lengths, POT_data):\n",
    "    print('\\n---------- Applying: %s\\n' % label)\n",
    "\n",
    "\n",
    "    #—————————————#\n",
    "    # Apply cuts  #\n",
    "    #—————————————#\n",
    "\n",
    "\n",
    "    if (label=='None'):                 # No selection applied\n",
    "        df_data = dfData\n",
    "        df_mc = dfOverlay\n",
    "        df_ext = dfEXT\n",
    "        df_dirt = dfDirt\n",
    "    elif (label=='recoVtxTrim'):         # Trim down badly reconstructed vertex values\n",
    "        df_data = reco_nu_vtx_val_clearing(dfData)\n",
    "        df_mc = reco_nu_vtx_val_clearing(dfOverlay)\n",
    "        df_ext = reco_nu_vtx_val_clearing(dfEXT)\n",
    "        df_dirt = reco_nu_vtx_val_clearing(dfDirt)\n",
    "    elif (label=='genNuSelection'):      # Generic Neutrino Selection and numuCC selection\n",
    "        df_data = gen_nu_selec(dfData)\n",
    "        df_mc = gen_nu_selec(dfOverlay)\n",
    "        df_ext = gen_nu_selec(dfEXT)\n",
    "        df_dirt = gen_nu_selec(dfDirt)\n",
    "    elif (label=='fiducialVol'):         # Fiducial volume selection\n",
    "        df_data = apply_inFV(dfData)\n",
    "        df_mc = apply_inFV(dfOverlay)\n",
    "        df_ext = apply_inFV(dfEXT)\n",
    "        df_dirt = apply_inFV(dfDirt)\n",
    "\n",
    "    if(print_lengths==True):\n",
    "        print('[DATA] %7i entries' % sum(df_data[weight]))\n",
    "        print('[EXT]  %7i entries' % sum(df_ext[weight]))\n",
    "        print('[MC]   %7i entries' % sum(df_mc[weight]))\n",
    "        print('[DIRT]   %7i entries' % sum(df_dirt[weight]))\n",
    "\n",
    "    # make stacked histograms\n",
    "    plots_for_SelectionCuts(df_data, df_ext, df_mc, df_dirt, weight, label, POT_data)\n",
    "        \n",
    "    # return the updated dataframes\n",
    "    return df_data, df_ext, df_mc, df_dirt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4f157",
   "metadata": {},
   "source": [
    "### MC Topology Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c7821b-362b-428f-816e-bb81787e776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCosmic(df):\n",
    "    df_ = df[((df.match_completeness_energy <= df.truth_energyInside * 0.1) \\\n",
    "               | (df.truth_energyInside <= 0))]\n",
    "    # print('Number of rows BEFORE query: %s' % len(df))\n",
    "    # df_ = df.query('match_completeness_energy <= truth_energyInside*0.1 \\\n",
    "    #                       | truth_energyInside <= 0')\n",
    "    # print('Number of rows AFTER query: %s' % len(df_))\n",
    "    return df_\n",
    "\n",
    "def notCosmic(df):\n",
    "    df_ = df[~((df.match_completeness_energy <= df.truth_energyInside * 0.1) \\\n",
    "               | (df.truth_energyInside <= 0))]\n",
    "    # print('Number of rows BEFORE query: %s' % len(df))\n",
    "    # df_ = df.query('match_completeness_energy > truth_energyInside*0.1 \\\n",
    "    #                & truth_energyInside > 0')\n",
    "    # print('Number of rows AFTER query: %s' % len(df_))\n",
    "    return df_\n",
    "\n",
    "def isOutFV(df):\n",
    "    df_ = df[(df.truth_vtxInside==0)]  \n",
    "    # print('Number of rows BEFORE query: %s' % len(df))\n",
    "    # df_ = df.query('truth_vtxInside==0')                          # vtx outFV\n",
    "    # print('Number of rows AFTER query: %s' % len(df_))\n",
    "    return df_\n",
    "\n",
    "def notOutFV(df):\n",
    "    df_ = df[(df.truth_vtxInside != 0)]\n",
    "    return df_\n",
    "\n",
    "def isNue_NuebarCC(df):\n",
    "    df_ = df[((df.truth_nuPdg==12) | (df.truth_nuPdg==-12))    \n",
    "             & (df.truth_isCC==1)] \n",
    "    # df_ = df.query('truth_nuPdg==12 | truth_nuPdg==-12')                            # nue/nue_barCC\n",
    "    return df_\n",
    "\n",
    "def isNumuCC(df):\n",
    "    df_ = df[(df.truth_nuPdg==14) &                             \n",
    "             (df.truth_isCC==1)] \n",
    "    # df_ = df.query('truth_nuPdg==14 & \\\n",
    "    #                truth_isCC==1')                               # numu CC\n",
    "    return df_\n",
    "\n",
    "def isNumu_barCC(df):\n",
    "    # df_ = df[(df.truth_nuPdg==-14) &                             \n",
    "    #          (df.truth_isCC==1)]  \n",
    "    df_ = df.query('truth_nuPdg==-14 & \\\n",
    "                   truth_isCC==1')                               # numu_barCC\n",
    "    return df_\n",
    "\n",
    "def isNCpi(df):\n",
    "    mask_pi = np.array([211 in array for array in np.abs(np.array(df.truth_pdg))])\n",
    "    mask_isNC = df.truth_isCC==0                                 # NC with charged pions\n",
    "    df_ = df[mask_isNC & mask_pi]\n",
    "    #df_ = df.query('(truth_pdg==211 | truth_pdg==-211) & truth_isCC==0')\n",
    "    return df_\n",
    "\n",
    "def isNC(df):\n",
    "    mask_no_pi = np.array([not 211 in array for array in np.abs(np.array(df.truth_pdg))])\n",
    "    mask_isNC = df.truth_isCC==0\n",
    "    df_ = df[mask_isNC & mask_no_pi]                             # NC with no pions\n",
    "    # df_ = df.query('(truth_pdg!=211 | truth_pdg!=-211) & truth_isCC==0')\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfaf661c-9ba3-4c5f-984c-ef61b0f3b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#—————————————————————\n",
    "#Test cell for mask_pi\n",
    "#—————————————————————\n",
    "\n",
    "# #criterion = []\n",
    "# for idx, subarray in enumerate(test_df[\"truth_pdg\"]):\n",
    "#     criterion.append(int((211) in np.abs(subarray)))\n",
    "\n",
    "# print(f\"Number of entries: {test_df['truth_pdg'].count()}\")\n",
    "# print(f\"Entries with 211: {sum(criterion)}\")\n",
    "# print('Number of NC: %s' % test_nc['truth_pdg'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5729120",
   "metadata": {},
   "source": [
    "### Funtions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c132fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================#\n",
    "#  Functions setting vars to plot after cuts  #\n",
    "#=============================================#\n",
    "\n",
    "\n",
    "def plots_for_SelectionCuts(df_data, df_ext, df_overlay, df_dirt, weight, label, POT_data):\n",
    "    \n",
    "    # this function makes all the plots that you want to plot throughout the pre-selection cuts\n",
    "    \n",
    "    # --- MC/DATA comparison\n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"reco_nuvtxX\", weight, \\\n",
    "                            \"plots/%s_nuvtxX\" % label, \"Reco neutrino Vertex X [cm]\", \\\n",
    "                            tpc_xmin, tpc_xmax, 25, POT_data)\n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"reco_nuvtxY\", weight, \\\n",
    "                            \"plots/%s_nuvtxY\" % label, \"Reco neutrino Vertex Y [cm]\", \\\n",
    "                            tpc_xmin, tpc_ymax, 25, POT_data)\n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"reco_nuvtxZ\", weight, \\\n",
    "                            \"plots/%s_nuvtxZ\" % label, \"Reco neutrino Vertex Z [cm]\", \\\n",
    "                            tpc_zmin, tpc_zmax, 25, POT_data)\n",
    "    \n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"flash_time\", weight, \\\n",
    "                            \"plots/%s_flash_time\" % label, r\"Flash Time [$\\mu$s]\", \\\n",
    "                            4, 18, 50, POT_data)\n",
    "    \n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"kine_reco_Enu\", weight, \\\n",
    "                            \"plots/%s_kine_reco_Enu\" % label, \"Reco Neutrino Energy [MeV]\",\\\n",
    "                            150, 2500, 35, POT_data)\n",
    "    \n",
    "    mc_data_stacked_hist(df_data, df_ext, df_overlay, df_dirt, \"numu_cc_flag\", weight, \\\n",
    "                            \"plots/%s_numu_cc_flag\" % label, r\"$\\nu_{\\mu}$ CC Score (AU)\",\\\n",
    "                            -1, 1, 10, POT_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef1bf039-7bed-43a1-be53-ff872760634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================#\n",
    "#  A Function for Plotting Stacked Histograms #\n",
    "#=============================================#\n",
    "\n",
    "\n",
    "def mc_data_stacked_hist(df_data, df_ext, df_mc, df_dirt, var, weight, plot_png_name, xaxis, xmin, \\\n",
    "                         xmax, nbins, pot_data):\n",
    "    #————————————————————————————————————————————————#\n",
    "    # Restrict the range of df in the plotting range #\n",
    "    #————————————————————————————————————————————————#\n",
    "    \n",
    "    \n",
    "    # df_data = df_data[(df_data[var]>=xmin) & (df_data[var]<=xmax)]\n",
    "    # df_ext = df_ext[(df_ext[var]>=xmin) & (df_ext[var]<=xmax)]\n",
    "    # df_mc = df_mc[(df_mc[var]>=xmin) & (df_mc[var]<=xmax)]\n",
    "    # df_dirt = df_dirt[(df_dirt[var]>=xmin) & (df_dirt[var]<=xmax)]\n",
    "    \n",
    "    \n",
    "    #————————————————————————————————————————————#\n",
    "    # Get the number of entries for each dataset #\n",
    "    #————————————————————————————————————————————#\n",
    "    \n",
    "    \n",
    "    n_data = sum(df_data[weight])\n",
    "    n_ext = sum(df_ext[weight])\n",
    "    n_mc = sum(df_mc[weight])\n",
    "    \n",
    "    #——————————————————————————————————————————————————————————————————————————#\n",
    "    # integrate +1 cm shift in MC reconstructed neutrino vertex in X direction #\n",
    "    #——————————————————————————————————————————————————————————————————————————#\n",
    "\n",
    "\n",
    "    # if (var == \"reco_nuvtxX\"):\n",
    "    #     df_mc.loc[:, var] = df_mc[var] + 1.\n",
    "    #     df_ext.loc[:, var] = df_ext[var] + 1.\n",
    "    #     df_dirt.loc[:, var] = df_dirt[var] + 1.\n",
    "\n",
    "    #———————————————————————————————————#\n",
    "    # Classify mc df into each topology #\n",
    "    #———————————————————————————————————#\n",
    "    \n",
    "    \n",
    "    # cosmic\n",
    "    df_cosmic = isCosmic(df_mc)\n",
    "    n_cosmic = sum(df_cosmic[weight])\n",
    "    df_mc = notCosmic(df_mc)\n",
    "\n",
    "    # outFV\n",
    "    df_outFV = isOutFV(df_mc)\n",
    "    n_outFV = sum(df_outFV[weight])\n",
    "    df_mc = notOutFV(df_mc)\n",
    "\n",
    "    # numuCC\n",
    "    df_numuCC = isNumuCC(df_mc)\n",
    "    n_numuCC = sum(df_numuCC[weight])\n",
    "\n",
    "    # numubarCC\n",
    "    df_numu_barCC = isNumu_barCC(df_mc)\n",
    "    n_numu_barCC = sum(df_numu_barCC[weight])\n",
    "\n",
    "    # NCpi+,-\n",
    "    df_ncpi = isNCpi(df_mc)\n",
    "    n_ncpi = sum(df_ncpi[weight])\n",
    "\n",
    "    # NC\n",
    "    df_nc = isNC(df_mc)\n",
    "    n_nc = sum(df_nc[weight])\n",
    "    \n",
    "    # nue and nuebarCC\n",
    "    df_nue_nuebarCC = isNue_NuebarCC(df_mc)\n",
    "    n_nue_nuebarCC = sum(df_nue_nuebarCC[weight])\n",
    "\n",
    "    # print('\\nOverlay (%i entries)' % n_mc)\n",
    "    # print('Cosmic    %10i' % n_cosmic)\n",
    "    # print('outFV     %10i \\n' % n_outFV)\n",
    "    # print('NumuCC %10i' % n_numuCC)\n",
    "    # print('NumubarCC    %10i' % n_numu_barCC)\n",
    "    # print('NCpi      %10i' % n_ncpi)\n",
    "    # print('NC        %10i' % n_nc)\n",
    "    # print('Nue/NuebarCC %10i' % n_nue_nuebarCC)\n",
    "    # print('Total     %10i' % (n_cosmic + n_outFV + n_numuCC + n_numu_barCC \\\n",
    "    #                           + n_ncpi + n_nc + n_nue_nuebarCC))\n",
    "\n",
    "\n",
    "    #—————————————#\n",
    "    # get entries #\n",
    "    #—————————————#\n",
    "    \n",
    "    \n",
    "    hist_list = [df_cosmic[var],\n",
    "                 df_outFV[var],\n",
    "                 df_numuCC[var],\n",
    "                 df_numu_barCC[var],\n",
    "                 df_nc[var],\n",
    "                 df_ncpi[var],\n",
    "                 df_nue_nuebarCC[var],\n",
    "                 df_ext[var],\n",
    "                 df_dirt[var]]\n",
    "    \n",
    "    hist_data = df_data[var]\n",
    "    \n",
    "\n",
    "    #—————————————#\n",
    "    # get weight  #\n",
    "    #—————————————#\n",
    "\n",
    "\n",
    "    w_list = [df_cosmic[weight],\n",
    "              df_outFV[weight],\n",
    "              df_numuCC[weight],\n",
    "              df_numu_barCC[weight],\n",
    "              df_nc[weight],\n",
    "              df_ncpi[weight],\n",
    "              df_nue_nuebarCC[weight],\n",
    "              df_ext[weight],\n",
    "              df_dirt[weight]]\n",
    "\n",
    "    w_data = df_data[weight]\n",
    "\n",
    "\n",
    "    #—————————————#\n",
    "    # Colors!!!!! #\n",
    "    #—————————————#\n",
    "    \n",
    "    \n",
    "    c_list = ['paleturquoise',\n",
    "              'springgreen',\n",
    "              'limegreen',\n",
    "              'aqua',\n",
    "              'orange',\n",
    "              'saddlebrown',\n",
    "              'grey',\n",
    "              'gold',\n",
    "              'pink']\n",
    "\n",
    "    c_data = 'black'\n",
    "    \n",
    "    \n",
    "    #————————#\n",
    "    # labels #\n",
    "    #————————#\n",
    "    \n",
    "    \n",
    "    label_list = ['Cosmic (%1.1f)'%(sum(w_list[0])),\n",
    "                  'outFV (%1.1f)'%(sum(w_list[1])),\n",
    "                  r'$\\nu_{\\mu}$ CC (%1.1f)'%(sum(w_list[2])),\n",
    "                  r'$\\bar{\\nu}_{\\mu}$ CC (%1.1f)'%(sum(w_list[3])),\n",
    "                  'NC (%1.1f)'%(sum(w_list[4])),\n",
    "                  r'NC $\\pi$ (%1.1f)'%(sum(w_list[5])),\n",
    "                  r'$\\nu_{e}$ CC (%1.1f)'%(sum(w_list[6])),\n",
    "                  'Beam Off (%1.1f)'%(sum(w_list[7])),\n",
    "                  'Out-of Cryo (%1.1f)'%(sum(w_list[8]))]\n",
    "\n",
    "    label_data = 'Beam-On (%1.1f)'%(sum(w_data))\n",
    "    \n",
    "    \n",
    "    #————————————————————————————#\n",
    "    # Calculate Stat Uncertainty #\n",
    "    #————————————————————————————#\n",
    "\n",
    "\n",
    "    # distributions without data pot normalisation\n",
    "    h_cosmicB, b_cosmicB = np.histogram(hist_list[0], bins=nbins, range=(xmin,xmax))\n",
    "    h_outFVB, b_outFVB = np.histogram(hist_list[1], bins=nbins, range=(xmin,xmax))\n",
    "    h_numuCCB, b_numuCCB = np.histogram(hist_list[2], bins=nbins, range=(xmin,xmax))\n",
    "    h_numuBarCCB, b_numuBarCCB = np.histogram(hist_list[3], bins=nbins, range=(xmin,xmax))\n",
    "    h_ncB, b_ncB = np.histogram(hist_list[4], bins=nbins, range=(xmin,xmax))\n",
    "    h_ncPiB, b_ncPiB = np.histogram(hist_list[5], bins=nbins, range=(xmin,xmax))\n",
    "    h_nueCCB, b_nueCCB = np.histogram(hist_list[6], bins=nbins, range=(xmin,xmax))\n",
    "    h_extB, b_extB = np.histogram(hist_list[7], bins=nbins, range=(xmin,xmax))\n",
    "    h_dirtB, b_dirtB = np.histogram(hist_list[8], bins=nbins, range=(xmin,xmax))\n",
    "\n",
    "    # distributions with data pot normalisation\n",
    "    h_cosmic, b_cosmic = np.histogram(hist_list[0], bins=nbins, range=(xmin,xmax), weights=w_list[0])\n",
    "    h_outFV, b_outFV = np.histogram(hist_list[1], bins=nbins, range=(xmin,xmax), weights=w_list[1])\n",
    "    h_numuCC, b_numuCC = np.histogram(hist_list[2], bins=nbins, range=(xmin,xmax), weights=w_list[2])\n",
    "    h_numuBarCC, b_numuBarCC = np.histogram(hist_list[3], bins=nbins, range=(xmin,xmax), weights=w_list[3])\n",
    "    h_nc, b_nc = np.histogram(hist_list[4], bins=nbins, range=(xmin,xmax), weights=w_list[4])\n",
    "    h_ncPi, b_ncPi = np.histogram(hist_list[5], bins=nbins, range=(xmin,xmax), weights=w_list[5])\n",
    "    h_nueCC, b_nueCC = np.histogram(hist_list[6], bins=nbins, range=(xmin,xmax), weights=w_list[6])\n",
    "    h_ext, b_ext = np.histogram(hist_list[7], bins=nbins, range=(xmin,xmax), weights=w_list[7])\n",
    "    h_dirt, b_dirt = np.histogram(hist_list[8], bins=nbins, range=(xmin,xmax), weights=w_list[8])\n",
    "\n",
    "    # sum mc distributions\n",
    "    total_mc = np.array([h_cosmic, h_outFV, h_numuCC, h_numuBarCC, h_nc, h_ncPi, h_nueCC, h_ext, h_dirt])\n",
    "    total_mc = total_mc.sum(axis=0)\n",
    "\n",
    "    # calculate their individual stat uncert as sqrt(number of entries) for the distribution before data pot normalisation\n",
    "    stat_cosmic = np.sqrt(h_cosmicB) \n",
    "    stat_outFV = np.sqrt(h_outFVB)\n",
    "    stat_numuCC = np.sqrt(h_numuCCB)\n",
    "    stat_numuBarCC = np.sqrt(h_numuBarCCB)\n",
    "    stat_nc = np.sqrt(h_ncB)\n",
    "    stat_ncPi = np.sqrt(h_ncPiB)\n",
    "    stat_nueCC = np.sqrt(h_nueCCB)\n",
    "    stat_ext = np.sqrt(h_extB)\n",
    "    stat_dirt = np.sqrt(h_dirtB)\n",
    "\n",
    "\n",
    "    # calculate ratio bin-by-bin of the distribution before/after data pot normalisation\n",
    "    def divide_arrays(arr1, arr2):\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            arr3 = np.true_divide(arr1, arr2)\n",
    "            arr3[arr3 == np.inf] = 0\n",
    "            arr3 = np.nan_to_num(arr3)\n",
    "        return arr3\n",
    "    ratio_cosmic = divide_arrays(h_cosmic, h_cosmicB)\n",
    "    ratio_outFV = divide_arrays(h_outFV, h_outFVB)\n",
    "    ratio_numuCC = divide_arrays(h_numuCC, h_numuCCB)\n",
    "    ratio_numuBarCC = divide_arrays(h_numuBarCC, h_numuBarCCB)\n",
    "    ratio_nc = divide_arrays(h_nc, h_ncB)\n",
    "    ratio_ncPi = divide_arrays(h_ncPi, h_ncPiB)\n",
    "    ratio_nueCC = divide_arrays(h_nueCC, h_nueCCB)\n",
    "    ratio_ext = divide_arrays(h_ext, h_extB)\n",
    "    ratio_dirt = divide_arrays(h_dirt, h_dirtB)\n",
    "\n",
    "    # update stat uncertainty by multiplying it by the ratio we've just calculated\n",
    "    stat_cosmic = np.multiply(stat_cosmic, ratio_cosmic)\n",
    "    stat_outFV = np.multiply(stat_outFV, ratio_outFV)\n",
    "    stat_numuCC = np.multiply(stat_numuCC, ratio_numuCC)\n",
    "    stat_numuBarCC = np.multiply(stat_numuBarCC, ratio_numuBarCC)\n",
    "    stat_nc = np.multiply(stat_nc, ratio_nc)\n",
    "    stat_ncPi = np.multiply(stat_ncPi, ratio_ncPi)\n",
    "    stat_nueCC = np.multiply(stat_nueCC, ratio_nueCC)\n",
    "    stat_ext = np.multiply(stat_ext, ratio_ext)\n",
    "    stat_dirt = np.multiply(stat_dirt, ratio_dirt)\n",
    "\n",
    "    # calculate total uncertainty\n",
    "    stat_cosmic2 = np.multiply(stat_cosmic, ratio_cosmic)\n",
    "    stat_outFV2 = np.multiply(stat_outFV, ratio_outFV)\n",
    "    stat_numuCC2 = np.multiply(stat_numuCC, ratio_numuCC)\n",
    "    stat_numuBarCC2 = np.multiply(stat_numuBarCC, ratio_numuBarCC)\n",
    "    stat_nc2 = np.multiply(stat_nc, ratio_nc)\n",
    "    stat_ncPi2 = np.multiply(stat_ncPi, ratio_ncPi)\n",
    "    stat_nueCC2 = np.multiply(stat_nueCC, ratio_nueCC)\n",
    "    stat_ext2 = np.multiply(stat_ext, ratio_ext)\n",
    "    stat_dirt2 = np.multiply(stat_dirt, ratio_dirt)\n",
    "\n",
    "    total_stat = np.array([stat_cosmic2, stat_outFV2, stat_numuCC2, stat_numuBarCC2, \n",
    "                          stat_nc2, stat_ncPi2,  stat_nueCC2, stat_ext2, stat_dirt2])\n",
    "    total_stat = np.sqrt(total_stat.sum(axis=0))\n",
    "\n",
    "\n",
    "    #——————————————————————#\n",
    "    # Create Stacked Histo #\n",
    "    #——————————————————————#\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8,10), gridspec_kw=dict(height_ratios=[4,1]), sharex=True)\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.1)\n",
    "    axs[0].hist(hist_list, bins=nbins, range=(xmin,xmax), weights=w_list, color=c_list, label=label_list, stacked=True)\n",
    "    h_data, b_data = np.histogram(hist_data, weights=w_data, bins=nbins, range=(xmin,xmax))\n",
    "    h_data_max = np.max(h_data)\n",
    "    err_bar = [np.sqrt(x) for x in h_data] # h_err = sqrt(bin)\n",
    "    mid = 0.5*(b_data[1:] + b_data[:-1])\n",
    "    axs[0].errorbar(mid, h_data, xerr=0.5*(xmax-xmin)/nbins, yerr=err_bar, color='black', label=label_data, fmt='o')\n",
    "    upvals = np.append((np.array(total_mc)+np.array(total_stat)),(np.array(total_mc)+np.array(total_stat))[-1])\n",
    "    lowvals = np.append((np.array(total_mc)-np.array(total_stat)),(np.array(total_mc)-np.array(total_stat))[-1])\n",
    "    axs[0].fill_between(b_data, lowvals, upvals, step='post', color='gray', hatch='///', alpha=0.3, zorder=2)\n",
    "    axs[0].legend(ncol=2, fontsize=12, frameon=False, loc='best')\n",
    "    axs[0].set_ylabel('Entries', size=15)\n",
    "    axs[0].set_title(\"MicroBooNE NuMI Data: 2.01e+20 POT\", size=15, loc='left')\n",
    "    axs[0].xaxis.set_tick_params(labelsize=15)\n",
    "    axs[0].yaxis.set_tick_params(labelsize=15)\n",
    "    axs[0].set_ylim([0, 1.3*h_data_max])\n",
    "    \n",
    "    axs[1].set_ylabel('Data/MC', size=15)\n",
    "    axs[1].set_xlabel('%s' % xaxis, size=15)\n",
    "    axs[1].xaxis.set_tick_params(labelsize=15)\n",
    "    axs[1].yaxis.set_tick_params(labelsize=15)\n",
    "    axs[1].set_ylim([0.5, 1.5])\n",
    "    \n",
    "    # --- bottom pad\n",
    "    \n",
    "    # calculate data/mc ratio and the uncertainty bar\n",
    "    ratio_data_mc = divide_arrays(h_data, total_mc)\n",
    "    axs[1].plot(mid, ratio_data_mc, marker='o', color='black', linewidth=0) # plot points\n",
    "    # draw data stat uncertainty\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num = np.sqrt(h_data)\n",
    "        den = total_mc\n",
    "        ratio_err_bar = np.true_divide(num, den)\n",
    "        ratio_err_bar[ratio_err_bar == np.inf] = 0\n",
    "        ratio_err_bar = np.nan_to_num(ratio_err_bar)\n",
    "    axs[1].errorbar(mid, ratio_data_mc, xerr=0.5*(xmax-xmin)/nbins, yerr=ratio_err_bar, color='black', label=label_data, fmt='o')\n",
    "    \n",
    "    # draw gray area error\n",
    "    ratio_center = np.ones(nbins)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num = total_stat\n",
    "        den = total_mc\n",
    "        ratio_err_area = np.true_divide(num, den)\n",
    "        ratio_err_area[ratio_err_area == np.inf] = 0\n",
    "        ratio_err_area = np.nan_to_num(ratio_err_area)\n",
    "    upvals = np.append((np.array(ratio_center)+np.array(ratio_err_area)),(np.array(ratio_center)+np.array(ratio_err_area))[-1])\n",
    "    lowvals = np.append((np.array(ratio_center)-np.array(ratio_err_area)),(np.array(ratio_center)-np.array(ratio_err_area))[-1])\n",
    "    axs[1].fill_between(b_data, lowvals, upvals, step='post', color='gray', alpha=0.3, zorder=2)\n",
    "    \n",
    "    plt.savefig('%s.png' % plot_png_name, dpi=600)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee261d0",
   "metadata": {},
   "source": [
    "### ACTUAL CODE EXECUTION\n",
    "\n",
    "First Get the files and create the dataframes for the NTuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d98d8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#—————————————————————————————————————#\n",
    "#  Get the files needed and their dir # \n",
    "#—————————————————————————————————————#\n",
    "file_mc = 'ROOT_files/checkout_prodgenie_numi_overlay_CV_run1.root'\n",
    "file_beam_on = 'ROOT_files/checkout_prodgenie_numi_beamon_run1.root'\n",
    "file_dirt = 'ROOT_files/run1_dirt.root'\n",
    "file_ext = 'ROOT_files/checkout_data_extnumi_run1.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bd1d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#———————————————————————————————#\n",
    "#Calculate pot for each dataset #\n",
    "#———————————————————————————————#\n",
    "    \n",
    "\n",
    "#From the run1 beam-on good quality file\n",
    "run1_tortgt_wcut = 2.014e+20\n",
    "run1_EA9CNT_wcut = 5304302.0\n",
    "\n",
    "#EXT info\n",
    "run1_EXT_NUMIwin_FEMBeamTriggerAlgo = 2466466.930000\n",
    "\n",
    "#calculate pot for datasets\n",
    "run1_pot_beam_on = run1_tortgt_wcut\n",
    "run1_pot_ext = run1_pot_beam_on/(run1_EA9CNT_wcut/run1_EXT_NUMIwin_FEMBeamTriggerAlgo)\n",
    "run1_pot_mc = calc_pot(file_mc, 'MC')\n",
    "run1_pot_dirt = calc_pot(file_dirt, 'DIRT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "995e48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#——————————————————————————————————————#\n",
    "# Create dataframes containing NTuples #\n",
    "#——————————————————————————————————————#\n",
    "\n",
    "\n",
    "lock1 = True\n",
    "if lock1==True:    \n",
    "    \n",
    "    df_mc = new_df(file_mc, 'MC', run1_pot_beam_on, run1_pot_ext, run1_pot_mc, run1_pot_dirt)\n",
    "    df_beam_on = new_df(file_beam_on, 'DATA', run1_pot_beam_on, run1_pot_ext, run1_pot_mc, run1_pot_dirt)\n",
    "    df_ext = new_df(file_ext, 'EXT', run1_pot_beam_on, run1_pot_ext, run1_pot_mc, run1_pot_dirt)\n",
    "    df_dirt = new_df(file_dirt, 'DIRT', run1_pot_beam_on, run1_pot_ext, run1_pot_mc, run1_pot_dirt)\n",
    "    print('\\nDataframes created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#——————————————————————————————————————#\n",
    "# Save Dataframe for fast code running #\n",
    "#——————————————————————————————————————#\n",
    "\n",
    "lock2 = False\n",
    "if lock2==True:  \n",
    "    df_mc.to_csv('dfs/df_run1_mc.csv')\n",
    "    df_beam_on.to_csv('dfs/df_run1_data.csv')\n",
    "    df_dirt.to_csv('dfs/df_run1_dirt.csv')\n",
    "    df_ext.to_csv('dfs/df_run1_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#———————————————————————————————————————#\n",
    "# Load Dataframes for fast code running #\n",
    "#———————————————————————————————————————#\n",
    "\n",
    "\n",
    "lock3 = False\n",
    "if lock3==True:\n",
    "    df_mc = pd.read_csv('dfs/df_run1_mc.csv')\n",
    "    df_beam_on = pd.read_csv('dfs/df_run1_data.csv')\n",
    "    df_dirt = pd.read_csv('dfs/df_run1_dirt.csv')\n",
    "    df_ext = pd.read_csv('dfs/df_run1_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————————————————————————————————————#\n",
    "# Now calculate particle multiplicity and merge them into df #\n",
    "#————————————————————————————————————————————————————————————#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#—————————————————————————————————————————————————————#\n",
    "# Now apply selection and draw histo and show entries #\n",
    "#—————————————————————————————————————————————————————#\n",
    "\n",
    "\n",
    "df_data1, df_ext1, df_mc1, df_dirt1 = applyCuts('None', df_beam_on, df_ext, df_mc, df_dirt, 'weight_ubtune', True, POT_data=run1_pot_beam_on)\n",
    "# df_data2, df_ext2, df_mc2, df_dirt2 = applyCuts('recoVtxTrim', df_data1, df_ext1, df_mc1, df_dirt1, 'weight_ubtune', True, POT_data=run1_pot_beam_on)\n",
    "df_data3, df_ext3, df_mc3, df_dirt3 = applyCuts('fiducialVol', df_data1, df_ext1, df_mc1, df_dirt1, 'weight_ubtune', True, POT_data=run1_pot_beam_on)\n",
    "df_data4, df_ext4, df_mc4, df_dirt4 = applyCuts('genNuSelection', df_data3, df_ext3, df_mc3, df_dirt3, 'weight_ubtune', True, POT_data=run1_pot_beam_on)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
